{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 303,
   "id": "c80cfc4c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Imports\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import math\n",
    "from collections import Counter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 304,
   "id": "e6f4b7f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Import dataset from CSV file\n",
    "column_types = {\n",
    "    \"Crash Date\": str,\n",
    "    \"Crash Time\": str,     \n",
    "    \"Borough\": str,      \n",
    "    \"Zip Code\": str,  \n",
    "    \"Latitude\": float,\n",
    "    \"Longitude\": float, \n",
    "    \"Location\": str,\n",
    "    \"On Street Name\": str, \n",
    "    \"Cross Street Name\": str, \n",
    "    \"Off Street Name\": str,         \n",
    "    \"Numbers of Persons injured\": int,      \n",
    "    \"Numbers of Persons killed\": int,         \n",
    "    \"Number of Pedestrians injured\": int,   \n",
    "    \"Number of Pedestrians killed\": int,         \n",
    "    \"Number of cyclists injured\": int,    \n",
    "    \"Number of cyclists killed\": int,         \n",
    "    \"Number of motorists injured\": int,   \n",
    "    \"Number of motorists killed\": int,       \n",
    "    \"Contributing Factor Vehicle 1\": str,       \n",
    "    \"Contributing Factor Vehicle 2\": str,  \n",
    "    \"Contributing Factor Vehicle 3\": str,      \n",
    "    \"Contributing Factor Vehicle 4\": str,    \n",
    "    \"Contributing Factor Vehicle 5\": str,\n",
    "    \"Collision ID\": int,\n",
    "    \"Vehicle Type Code 1\": str,\n",
    "    \"Vehicle Type Code 2\": str,\n",
    "    \"Vehicle Type Code 3\": str,\n",
    "    \"Vehicle Type Code 4\": str,\n",
    "    \"Vehicle Type Code 5\": str\n",
    "}\n",
    "#crashes = pd.read_csv(\"collisions.csv\", dtype=column_types, low_memory=False)\n",
    "path = '/kaggle/input/motor-vehicle-collisions/Motor_Vehicle_Collisions_-_Crashes.csv'\n",
    "crashes = pd.read_csv(path, dtype=column_types, low_memory=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 305,
   "id": "d9219747",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "500\n",
      "635\n"
     ]
    }
   ],
   "source": [
    "#ADJUST VALUES TO IMPACT MODEL\n",
    "#Define Grid over New York City\n",
    "#Distance between these two points is about 71.43km,\n",
    "#so about 50km lateral and 50km longitudinal\n",
    "south_west_corner = (40.485347, -74.276931)\n",
    "north_east_corner = (40.948379, -73.689515)\n",
    "\n",
    "#Define grid properties\n",
    "lat_density = 1000 #Represents the number of grid cells for latitude, ~50km / 1000 = ~50m between each grid point\n",
    "grid_dist = (north_east_corner[0] - south_west_corner[0]) / lat_density\n",
    "lon_density = math.ceil((north_east_corner[1] - south_west_corner[1]) / grid_dist)\n",
    "print(lat_density)\n",
    "print(lon_density)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 322,
   "id": "845adee8",
   "metadata": {},
   "outputs": [],
   "source": [
    "#For testing, remove when training\n",
    "#print(len(crashes))\n",
    "#crashes = crashes.head(100000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 337,
   "id": "1de629c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Fitlter rows without latitude and longitude values or 0 values\n",
    "crashes = crashes.dropna(subset=['LATITUDE', 'LONGITUDE'])\n",
    "crashes = crashes[(crashes['LATITUDE'] != 0) & (crashes['LONGITUDE'] != 0)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 343,
   "id": "5453ec17",
   "metadata": {},
   "outputs": [],
   "source": [
    "#For progress tracking in next step\n",
    "progress_density = 10\n",
    "progress = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 344,
   "id": "a88e273f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Progress:  0 %\n",
      "Progress:  10 %\n",
      "Progress:  20 %\n",
      "Progress:  30 %\n",
      "Progress:  40 %\n",
      "Progress:  50 %\n",
      "Progress:  60 %\n",
      "Progress:  70 %\n",
      "Progress:  80 %\n",
      "Progress:  90 %\n",
      "Progress:  100 %\n",
      "Finsihed\n"
     ]
    }
   ],
   "source": [
    "#Map lat and long in the dataset to the closest point on the grid\n",
    "crash_counts = np.zeros((lat_density, lon_density), dtype=int)\n",
    "for i, row in crashes.iterrows():\n",
    "    lat_i = round((row['LATITUDE'] - south_west_corner[0]) / grid_dist)\n",
    "    lon_i = round((row['LONGITUDE'] - south_west_corner[1]) / grid_dist)\n",
    "    if (lat_i > 0 and lat_i < lat_density and lon_i > 0 and lon_i < lon_density):\n",
    "        crash_counts[lat_i][lon_i]+=1\n",
    "       \n",
    "    #Progress tracking because this takes a while sometimes\n",
    "#    if (i > (len(crashes) / (100/progress_density)) * progress and progress * progress_density <= 100):\n",
    "#        print(\"Progress: \", progress*progress_density, \"%\")\n",
    "#        progress += 1        \n",
    "#print(\"Finsihed\")\n",
    "#progress = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 277,
   "id": "273e728f",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#ADJUST VALUES TO IMPACT MODEL\n",
    "#Define validation grid\n",
    "val_dist = 3        #Distance between validation points\n",
    "val_offset_lat = 0  #Offset of first validation point latitude wise\n",
    "val_offset_lon = 0  #Offset of first validation point longitude wise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 278,
   "id": "ba7e6b32",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of validation points latitude:  167\n",
      "Number of validation points longitude:  212\n",
      "Percentage of points used for validation 0.11150866141732284\n"
     ]
    }
   ],
   "source": [
    "#For debugging, this value represenets the percentage of grid points that will be used for validation\n",
    "#Add 2 to density\n",
    "val_points_lat = ((lat_density + val_dist - val_offset_lat - 1) // val_dist)\n",
    "val_points_lon = ((lon_density + val_dist - val_offset_lon - 1) // val_dist)\n",
    "val_percentage = (val_points_lat * val_points_lon) / (lat_density * lon_density)\n",
    "print(\"Number of validation points latitude: \", val_points_lat)\n",
    "print(\"Number of validation points longitude: \", val_points_lon)\n",
    "print(\"Percentage of points used for validation\", val_percentage)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 279,
   "id": "174aa368",
   "metadata": {},
   "outputs": [],
   "source": [
    "features     = []\n",
    "labels       = []\n",
    "features_val = []\n",
    "labels_val   = []\n",
    "\n",
    "\n",
    "for lat_i, row in enumerate(crash_counts):\n",
    "    for lon_i, count in enumerate(row):\n",
    "        lat = south_west_corner[0] + lat_i * grid_dist\n",
    "        lon = south_west_corner[1] + lon_i * grid_dist\n",
    "        \n",
    "        if ((lat_i + val_dist - val_offset_lat) % val_dist == 0) and ((lon_i + val_dist - val_offset_lon) % val_dist == 0):\n",
    "            features_val.append((lat, lon))\n",
    "            labels_val.append(count)\n",
    "        else:\n",
    "            features.append((lat, lon))\n",
    "            labels.append(count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 346,
   "id": "eebabc8f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Lenght of validation feature array:  35404\n",
      "Lenght of training feature array:  282096\n",
      "Percentage of validation featrue grid point:  0.11150866141732284\n",
      "Percentage of training featrue grid point:  0.8884913385826771\n"
     ]
    }
   ],
   "source": [
    "#Debugging\n",
    "print(\"Lenght of validation feature array: \", len(features_val))\n",
    "print(\"Lenght of training feature array: \", len(features))\n",
    "print(\"Percentage of validation featrue grid point: \", len(features_val) / (lat_density * lon_density))\n",
    "print(\"Percentage of training featrue grid point: \", len(features) / (lat_density * lon_density))\n",
    "\n",
    "#print(\"Validation features and labels:\")\n",
    "#for i, _ in enumerate(features_val):\n",
    "#    print(features_val[i][0], features_val[i][1], labels_val[i])\n",
    "\n",
    "#print(\"Training features and labels:\")\n",
    "#for i, _ in enumerate(features):\n",
    "#    print(features[i][0], features[i][1], labels[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 281,
   "id": "155e4be8",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'tensorflow'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[281], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;66;03m#Import tensorflow\u001b[39;00m\n\u001b[1;32m----> 2\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mtf\u001b[39;00m\n\u001b[0;32m      3\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m keras\n\u001b[0;32m      4\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mkeras\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m layers\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'tensorflow'"
     ]
    }
   ],
   "source": [
    "#Import tensorflow\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85d56cb3",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = keras.models.Sequential()\n",
    "model.add(keras.layers.Dense(64, activation='relu', input_shape=(2,)))\n",
    "model.add(keras.layers.Dense(64, activation='relu'))\n",
    "model.add(keras.layers.Dense(64, activation='relu'))\n",
    "model.add(keras.layers.Dense(1, activation='linear')) #Linear loss function because we are using the amount of crashes as labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d9479e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(optimizer='adam', loss='mean_squared_error')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3cd1514",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Train model\n",
    "model.fit(np.array(features), np.array(labels), epochs=5, batch_size=64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d89e3c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Evaluate model\n",
    "model.evaluate(np.array(features_val), np.array(labels_val))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
