{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "de524028",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.compose import ColumnTransformer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0a50bd34",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Read dataset\n",
    "df = pd.read_csv(\"collisions.csv\", low_memory=False)\n",
    "\n",
    "#Contained Columns:\n",
    "#CRASH DATE,CRASH TIME,BOROUGH,ZIP CODE,LATITUDE,LONGITUDE,LOCATION,ON STREET NAME,CROSS STREET NAME,OFF STREET NAME,\n",
    "#NUMBER OF PERSONS INJURED,NUMBER OF PERSONS KILLED,NUMBER OF PEDESTRIANS INJURED,NUMBER OF PEDESTRIANS KILLED,\n",
    "#NUMBER OF CYCLIST INJURED,NUMBER OF CYCLIST KILLED,NUMBER OF MOTORIST INJURED,NUMBER OF MOTORIST KILLED,\n",
    "#CONTRIBUTING FACTOR VEHICLE 1,CONTRIBUTING FACTOR VEHICLE 2,CONTRIBUTING FACTOR VEHICLE 3,\n",
    "#CONTRIBUTING FACTOR VEHICLE 4,CONTRIBUTING FACTOR VEHICLE 5,\n",
    "#COLLISION_ID,\n",
    "#VEHICLE TYPE CODE 1,VEHICLE TYPE CODE 2,VEHICLE TYPE CODE 3,VEHICLE TYPE CODE 4,VEHICLE TYPE CODE 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "42b45f59",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Preprocess the data\n",
    "\n",
    "#rename columns\n",
    "df = df.rename(columns={'CRASH DATE' : 'Date', 'CRASH TIME' : 'Time', 'ON STREET NAME' : 'Street', \n",
    "                        'VEHICLE TYPE CODE 1' : 'Vehicle 1', 'VEHICLE TYPE CODE 2' : 'Vehicle 2',\n",
    "                        'VEHICLE TYPE CODE 3' : 'Vehicle 3', 'VEHICLE TYPE CODE 4' : 'Vehicle 4',\n",
    "                        'VEHICLE TYPE CODE 5' : 'Vehicle 5', 'NUMBER OF PERSONS INJURED':'Injuries', \n",
    "                        'NUMBER OF PERSONS KILLED': 'Deaths'})\n",
    "\n",
    "# Keep rows fith defined 'street', 'crash time' and Vehicle Type 1\n",
    "df = df.dropna(subset=['Street', 'Time', 'Vehicle 1'])\n",
    "\n",
    "# Preprocess the date column into timestamps\n",
    "df['Date'] = pd.to_datetime(df['Date'])\n",
    "df['Date'] = (df['Date'] - pd.Timestamp(\"2010-01-01\")) // pd.Timedelta('1s')\n",
    "\n",
    "# Preprocess the time column into minutes from midnight\n",
    "df['Time'] = pd.to_datetime(df['Time'], format='%H:%M').dt.time\n",
    "df['Time'] = df['Time'].apply(lambda t: t.hour * 60 + t.minute)\n",
    "\n",
    "# Format dataset for uniform strings\n",
    "df['Street'] = df['Street'].str.upper()\n",
    "df['Vehicle 1'] = df['Vehicle 1'].str.upper()\n",
    "df['Vehicle 2'] = df['Vehicle 2'].str.upper()\n",
    "df['Vehicle 3'] = df['Vehicle 3'].str.upper()\n",
    "df['Vehicle 4'] = df['Vehicle 4'].str.upper()\n",
    "df['Vehicle 5'] = df['Vehicle 5'].str.upper()\n",
    "\n",
    "##List of columns we keep\n",
    "columns_to_keep = ['Date', 'Time', 'Street', 'Vehicle 1', 'Vehicle 2', 'Vehicle 3', 'Vehicle 4','Vehicle 5','Injuries','Deaths']\n",
    "df = df[columns_to_keep]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "7494b52e",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "#Limit dataframe size\n",
    "new_df = pd.DataFrame.from_records(df[:4000])  \n",
    "\n",
    "\n",
    "# Create a list of the columns that represent vehicle types\n",
    "vehicle_columns = ['Vehicle 1', 'Vehicle 2', 'Vehicle 3', 'Vehicle 4','Vehicle 5']\n",
    "\n",
    "def count_vehicles_not_in(row, exclude_set):\n",
    "    count = 0\n",
    "    for vehicle in row:\n",
    "        if pd.isna(vehicle):\n",
    "            continue\n",
    "        vehicle_upper = str(vehicle).upper()\n",
    "        if vehicle_upper not in exclude_set:\n",
    "            count += 1\n",
    "    return count\n",
    "\n",
    "def count_vehicles_in(row, exclude_set):\n",
    "    count = 0\n",
    "    for vehicle in row:\n",
    "        if pd.isna(vehicle):\n",
    "            continue\n",
    "        vehicle_upper = str(vehicle).upper()\n",
    "        if vehicle_upper in exclude_set:\n",
    "            count += 1\n",
    "    return count\n",
    "\n",
    "new_df['CARS_INVOLVED'] = new_df[vehicle_columns].apply(count_vehicles_not_in, args= ({'BIKE', 'E-BIKE', 'EBIKE', 'E-SCOOTER', 'ESCOOTER', 'BICYCLE', 'MOTORCYCLE', 'MOTORSCOOTER', 'MOPED'},), axis=1)\n",
    "new_df['BIKES_INVOLVED'] = new_df[vehicle_columns].apply(count_vehicles_in, args= ({'BIKE', 'E-BIKE', 'EBIKE', 'E-SCOOTER', 'ESCOOTER', 'BICYCLE'},), axis=1)\n",
    "new_df['MOTORBIKES_INVOLVED'] = new_df[vehicle_columns].apply(count_vehicles_in, args= ({'MOTORCYCLE', 'MOTORSCOOTER', 'MOPED'},), axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "960ed45d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "           Date  Time                        Street  CARS_INVOLVED  \\\n",
      "0     337478400   159         WHITESTONE EXPRESSWAY              2   \n",
      "1     322876800   705       QUEENSBORO BRIDGE UPPER              1   \n",
      "2     331084800   415            THROGS NECK BRIDGE              2   \n",
      "3     324518400   767  MAJOR DEEGAN EXPRESSWAY RAMP              2   \n",
      "4     345600000  1025    BROOKLYN QUEENS EXPRESSWAY              2   \n",
      "...         ...   ...                           ...            ...   \n",
      "3995  325382400  1110               WEST 137 STREET              1   \n",
      "3996  325382400   860                      65 PLACE              2   \n",
      "3997  325296000  1035               WEST 124 STREET              1   \n",
      "3998  325382400     1                     79 STREET              1   \n",
      "3999  325382400  1297                EAST 34 STREET              2   \n",
      "\n",
      "      BIKES_INVOLVED  MOTORBIKES_INVOLVED  Injuries  Deaths  \n",
      "0                  0                    0       2.0     0.0  \n",
      "1                  0                    0       1.0     0.0  \n",
      "2                  0                    0       0.0     0.0  \n",
      "3                  0                    0       0.0     0.0  \n",
      "4                  0                    0       0.0     0.0  \n",
      "...              ...                  ...       ...     ...  \n",
      "3995               0                    0       0.0     0.0  \n",
      "3996               0                    0       2.0     0.0  \n",
      "3997               1                    0       1.0     0.0  \n",
      "3998               0                    0       0.0     0.0  \n",
      "3999               0                    0       0.0     0.0  \n",
      "\n",
      "[4000 rows x 8 columns]\n"
     ]
    }
   ],
   "source": [
    "# Now the 'new_df' DataFrame contains the transformed data\n",
    "needed_columns = ['Date', 'Time', 'Street','CARS_INVOLVED','BIKES_INVOLVED', 'MOTORBIKES_INVOLVED','Injuries', 'Deaths']\n",
    "\n",
    "new_df = new_df[needed_columns]\n",
    "\n",
    "print(new_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "724eba8d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the scoring mechanism for each vehicle type\n",
    "def calculate_vehicle_score(row, vehicle_type):\n",
    "    involved_vehicles = row[f'{vehicle_type}_INVOLVED']\n",
    "    injuries = row['Injuries']\n",
    "    deaths = row['Deaths']\n",
    "    \n",
    "    # Define the weights for the scoring mechanism\n",
    "    vehicle_weight = {'CARS': 0.5, 'BIKES': 0.3, 'MOTORBIKES': 0.2}\n",
    "    injury_weight = 0.2\n",
    "    death_weight = 0.3\n",
    "    \n",
    "    # Calculate the vehicle score\n",
    "    vehicle_score = (involved_vehicles / new_df[f'{vehicle_type}_INVOLVED'].sum()) * 100 * vehicle_weight[vehicle_type]\n",
    "    \n",
    "    # Calculate the injury score\n",
    "    injury_score = (injuries / new_df['Injuries'].sum()) * 100 * injury_weight\n",
    "    \n",
    "    # Calculate the death score\n",
    "    death_score = (deaths / new_df['Deaths'].sum()) * 100 * death_weight\n",
    "    \n",
    "    # Calculate the total score\n",
    "    total_score = vehicle_score + injury_score + death_score\n",
    "    \n",
    "    return total_score\n",
    "\n",
    "# Create a list of vehicle types\n",
    "vehicle_types = ['CARS', 'BIKES', 'MOTORBIKES']\n",
    "\n",
    "# Calculate the score for each vehicle type and create new columns\n",
    "for vehicle_type in vehicle_types:\n",
    "    new_df[f'{vehicle_type}_DANGERSCORE'] = new_df.apply(calculate_vehicle_score, args=(vehicle_type,), axis=1)\n",
    "\n",
    "# Now the DataFrame contains new columns with the scores for each vehicle type (e.g., 'cars_score', 'bikes_score', 'motorbikes_score')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "6f7d565e",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "           Date  Time              Street  CARS_INVOLVED  BIKES_INVOLVED  \\\n",
      "2998  345772800    15  MORRIS PARK AVENUE              1               0   \n",
      "\n",
      "      MOTORBIKES_INVOLVED  Injuries  Deaths  CARS_DANGERSCORE  \\\n",
      "2998                    0       0.0     2.0          3.165124   \n",
      "\n",
      "      BIKES_DANGERSCORE  MOTORBIKES_DANGERSCORE  \n",
      "2998           3.157895                3.157895  \n"
     ]
    }
   ],
   "source": [
    "print(new_df[new_df['CARS_DANGERSCORE'] > 3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "b02cca20",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "           Date  Time                        Street  CARS_DANGERSCORE  \\\n",
      "0     337478400   159         WHITESTONE EXPRESSWAY          0.032825   \n",
      "1     322876800   705       QUEENSBORO BRIDGE UPPER          0.016412   \n",
      "2     331084800   415            THROGS NECK BRIDGE          0.014459   \n",
      "3     324518400   767  MAJOR DEEGAN EXPRESSWAY RAMP          0.014459   \n",
      "4     345600000  1025    BROOKLYN QUEENS EXPRESSWAY          0.014459   \n",
      "...         ...   ...                           ...               ...   \n",
      "3995  325382400  1110               WEST 137 STREET          0.007230   \n",
      "3996  325382400   860                      65 PLACE          0.032825   \n",
      "3997  325296000  1035               WEST 124 STREET          0.016412   \n",
      "3998  325382400     1                     79 STREET          0.007230   \n",
      "3999  325382400  1297                EAST 34 STREET          0.014459   \n",
      "\n",
      "      BIKES_DANGERSCORE  MOTORBIKES_DANGERSCORE  \n",
      "0              0.018365                0.018365  \n",
      "1              0.009183                0.009183  \n",
      "2              0.000000                0.000000  \n",
      "3              0.000000                0.000000  \n",
      "4              0.000000                0.000000  \n",
      "...                 ...                     ...  \n",
      "3995           0.000000                0.000000  \n",
      "3996           0.018365                0.018365  \n",
      "3997           0.090704                0.009183  \n",
      "3998           0.000000                0.000000  \n",
      "3999           0.000000                0.000000  \n",
      "\n",
      "[4000 rows x 6 columns]\n"
     ]
    }
   ],
   "source": [
    "needed_columns= ['Date', 'Time', 'Street','CARS_DANGERSCORE','BIKES_DANGERSCORE','MOTORBIKES_DANGERSCORE']\n",
    "\n",
    "df_for_model = pd.DataFrame.from_records(new_df[needed_columns])\n",
    "\n",
    "print(df_for_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "40b60579",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\maxsk\\anaconda3\\lib\\site-packages\\sklearn\\preprocessing\\_encoders.py:828: FutureWarning: `sparse` was renamed to `sparse_output` in version 1.2 and will be removed in 1.4. `sparse_output` is ignored unless you leave `sparse` to its default value.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "ename": "MemoryError",
     "evalue": "Unable to allocate 179. GiB for an array with shape (1562181, 15359) and data type float64",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mMemoryError\u001b[0m                               Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[9], line 5\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;66;03m# Preprocess the data\u001b[39;00m\n\u001b[0;32m      2\u001b[0m \n\u001b[0;32m      3\u001b[0m \u001b[38;5;66;03m# Perform one-hot encoding for 'Street' column\u001b[39;00m\n\u001b[0;32m      4\u001b[0m street_encoder \u001b[38;5;241m=\u001b[39m OneHotEncoder(sparse\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m, drop\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mfirst\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m----> 5\u001b[0m street_encoded \u001b[38;5;241m=\u001b[39m \u001b[43mstreet_encoder\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit_transform\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdf\u001b[49m\u001b[43m[\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mStreet\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m      6\u001b[0m street_columns \u001b[38;5;241m=\u001b[39m street_encoder\u001b[38;5;241m.\u001b[39mget_feature_names([\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mStreet\u001b[39m\u001b[38;5;124m'\u001b[39m])\n\u001b[0;32m      8\u001b[0m df_encoded \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mconcat([df\u001b[38;5;241m.\u001b[39mdrop(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mStreet\u001b[39m\u001b[38;5;124m'\u001b[39m, axis\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m), pd\u001b[38;5;241m.\u001b[39mDataFrame(street_encoded, columns\u001b[38;5;241m=\u001b[39mstreet_columns)], axis\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m)\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\sklearn\\utils\\_set_output.py:142\u001b[0m, in \u001b[0;36m_wrap_method_output.<locals>.wrapped\u001b[1;34m(self, X, *args, **kwargs)\u001b[0m\n\u001b[0;32m    140\u001b[0m \u001b[38;5;129m@wraps\u001b[39m(f)\n\u001b[0;32m    141\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mwrapped\u001b[39m(\u001b[38;5;28mself\u001b[39m, X, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m--> 142\u001b[0m     data_to_wrap \u001b[38;5;241m=\u001b[39m f(\u001b[38;5;28mself\u001b[39m, X, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m    143\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(data_to_wrap, \u001b[38;5;28mtuple\u001b[39m):\n\u001b[0;32m    144\u001b[0m         \u001b[38;5;66;03m# only wrap the first output for cross decomposition\u001b[39;00m\n\u001b[0;32m    145\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m (\n\u001b[0;32m    146\u001b[0m             _wrap_data_with_container(method, data_to_wrap[\u001b[38;5;241m0\u001b[39m], X, \u001b[38;5;28mself\u001b[39m),\n\u001b[0;32m    147\u001b[0m             \u001b[38;5;241m*\u001b[39mdata_to_wrap[\u001b[38;5;241m1\u001b[39m:],\n\u001b[0;32m    148\u001b[0m         )\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\sklearn\\base.py:859\u001b[0m, in \u001b[0;36mTransformerMixin.fit_transform\u001b[1;34m(self, X, y, **fit_params)\u001b[0m\n\u001b[0;32m    855\u001b[0m \u001b[38;5;66;03m# non-optimized default implementation; override when a better\u001b[39;00m\n\u001b[0;32m    856\u001b[0m \u001b[38;5;66;03m# method is possible for a given clustering algorithm\u001b[39;00m\n\u001b[0;32m    857\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m y \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    858\u001b[0m     \u001b[38;5;66;03m# fit method of arity 1 (unsupervised transformation)\u001b[39;00m\n\u001b[1;32m--> 859\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mfit_params\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtransform\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    860\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    861\u001b[0m     \u001b[38;5;66;03m# fit method of arity 2 (supervised transformation)\u001b[39;00m\n\u001b[0;32m    862\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfit(X, y, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mfit_params)\u001b[38;5;241m.\u001b[39mtransform(X)\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\sklearn\\utils\\_set_output.py:142\u001b[0m, in \u001b[0;36m_wrap_method_output.<locals>.wrapped\u001b[1;34m(self, X, *args, **kwargs)\u001b[0m\n\u001b[0;32m    140\u001b[0m \u001b[38;5;129m@wraps\u001b[39m(f)\n\u001b[0;32m    141\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mwrapped\u001b[39m(\u001b[38;5;28mself\u001b[39m, X, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m--> 142\u001b[0m     data_to_wrap \u001b[38;5;241m=\u001b[39m f(\u001b[38;5;28mself\u001b[39m, X, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m    143\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(data_to_wrap, \u001b[38;5;28mtuple\u001b[39m):\n\u001b[0;32m    144\u001b[0m         \u001b[38;5;66;03m# only wrap the first output for cross decomposition\u001b[39;00m\n\u001b[0;32m    145\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m (\n\u001b[0;32m    146\u001b[0m             _wrap_data_with_container(method, data_to_wrap[\u001b[38;5;241m0\u001b[39m], X, \u001b[38;5;28mself\u001b[39m),\n\u001b[0;32m    147\u001b[0m             \u001b[38;5;241m*\u001b[39mdata_to_wrap[\u001b[38;5;241m1\u001b[39m:],\n\u001b[0;32m    148\u001b[0m         )\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\sklearn\\preprocessing\\_encoders.py:918\u001b[0m, in \u001b[0;36mOneHotEncoder.transform\u001b[1;34m(self, X)\u001b[0m\n\u001b[0;32m    912\u001b[0m out \u001b[38;5;241m=\u001b[39m sparse\u001b[38;5;241m.\u001b[39mcsr_matrix(\n\u001b[0;32m    913\u001b[0m     (data, indices, indptr),\n\u001b[0;32m    914\u001b[0m     shape\u001b[38;5;241m=\u001b[39m(n_samples, feature_indices[\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m]),\n\u001b[0;32m    915\u001b[0m     dtype\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdtype,\n\u001b[0;32m    916\u001b[0m )\n\u001b[0;32m    917\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msparse_output:\n\u001b[1;32m--> 918\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mout\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtoarray\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    919\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    920\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m out\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\scipy\\sparse\\_compressed.py:1051\u001b[0m, in \u001b[0;36m_cs_matrix.toarray\u001b[1;34m(self, order, out)\u001b[0m\n\u001b[0;32m   1049\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m out \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m order \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m   1050\u001b[0m     order \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_swap(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcf\u001b[39m\u001b[38;5;124m'\u001b[39m)[\u001b[38;5;241m0\u001b[39m]\n\u001b[1;32m-> 1051\u001b[0m out \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_process_toarray_args\u001b[49m\u001b[43m(\u001b[49m\u001b[43morder\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mout\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1052\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (out\u001b[38;5;241m.\u001b[39mflags\u001b[38;5;241m.\u001b[39mc_contiguous \u001b[38;5;129;01mor\u001b[39;00m out\u001b[38;5;241m.\u001b[39mflags\u001b[38;5;241m.\u001b[39mf_contiguous):\n\u001b[0;32m   1053\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mOutput array must be C or F contiguous\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\scipy\\sparse\\_base.py:1298\u001b[0m, in \u001b[0;36mspmatrix._process_toarray_args\u001b[1;34m(self, order, out)\u001b[0m\n\u001b[0;32m   1296\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m out\n\u001b[0;32m   1297\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1298\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mnp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mzeros\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mshape\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdtype\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdtype\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43morder\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43morder\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[1;31mMemoryError\u001b[0m: Unable to allocate 179. GiB for an array with shape (1562181, 15359) and data type float64"
     ]
    }
   ],
   "source": [
    "# Preprocess the data\n",
    "\n",
    "# Perform one-hot encoding for 'Street' column\n",
    "# TODO: FIX ENCODING!!!!\n",
    "street_encoder = OneHotEncoder(sparse=False, drop='first')\n",
    "street_encoded = street_encoder.fit_transform(df[['Street']])\n",
    "street_columns = street_encoder.get_feature_names(['Street'])\n",
    "\n",
    "df_encoded = pd.concat([df.drop('Street', axis=1), pd.DataFrame(street_encoded, columns=street_columns)], axis=1)\n",
    "\n",
    "# Train separate linear regression models for each vehicle type\n",
    "car_data = df_for_model[['Time', 'Date', 'Street', 'CARS_DANGERSCORE']].dropna()\n",
    "car_X = car_data[['Time', 'Date', 'Street']]\n",
    "car_y = car_data['CARS_DANGERSCORE']\n",
    "car_model = LinearRegression()\n",
    "car_model.fit(car_X, car_y)\n",
    "\n",
    "bike_data = df_for_model[['Time', 'Date', 'Street', 'BIKES_DANGERSCORE']].dropna()\n",
    "bike_X = bike_data[['Time', 'Date', 'Street']]\n",
    "bike_y = bike_data['BIKES_DANGERSCORE']\n",
    "bike_model = LinearRegression()\n",
    "bike_model.fit(bike_X, bike_y)\n",
    "\n",
    "motorbike_data = df_for_model[['Time', 'Date', 'Street', 'MOTORBIKES_DANGERSCORE']].dropna()\n",
    "motorbike_X = motorbike_data[['Time', 'Date', 'Street']]\n",
    "motorbike_y = motorbike_data['MOTORBIKES_DANGERSCORE']\n",
    "motorbike_model = LinearRegression()\n",
    "motorbike_model.fit(motorbike_X, motorbike_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5153bb0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to predict the danger score for a given combination of 'Time', 'Date', 'Street' and vehicle type\n",
    "def predict_danger_score(time, date, street, vehicle_type):\n",
    "    # TODO: FIX ENCODING!!!!\n",
    "    street_encoded = street_encoder.transform([[street]])\n",
    "    street_columns = street_encoder.get_feature_names(['Street'])\n",
    "    street_df = pd.DataFrame(street_encoded, columns=street_columns)\n",
    "    \n",
    "    data = [[time, date, *street_df.values[0]]]\n",
    "    \n",
    "    if vehicle_type == 'cars':\n",
    "        model = car_model\n",
    "    elif vehicle_type == 'bikes':\n",
    "        model = bike_model\n",
    "    elif vehicle_type == 'motorbikes':\n",
    "        model = motorbike_model\n",
    "    else:\n",
    "        return None\n",
    "    \n",
    "    return model.predict(data)[0]\n",
    "\n",
    "# Example usage\n",
    "time = 14  # Example value for 'Time'\n",
    "date = pd.to_datetime('2023-07-07')  # Example value for 'Date'\n",
    "street = 'BROADWAY'  # Example value for 'Street'\n",
    "vehicle_type = 'cars'  # Example vehicle type\n",
    "\n",
    "predicted_score = predict_danger_score(time, date, street, vehicle_type)\n",
    "print(f\"The predicted danger score for {vehicle_type} at {time}:00 on {date.date()} on {street} is {predicted_score}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51162687",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
